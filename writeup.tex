\documentclass[11pt]{article}
\usepackage{common}
\title{CS 182 Final Project: Flaippy Bird}
\author{Joseph Kahn, Caetano Hanta-Davis, and Aron Szanto}
\begin{document}
\maketitle{}


\section{Introduction}

% A description of the purpose, goals, and scope of your system or
% empirical investigation.  You should include references to papers you
% read on which your project and any algorithms you used are
% based. Include a discussion of whether you adapted a published
% algorithm or devised a new one, the range of problems and issues you
% addressed, and the relation of these problems and issues to the
% techniques and ideas covered in the course.

This project was a technical study of the game ``Flappy Bird'' (FB) through the lens
of artificial intelligence. Using both deterministic and randomized paths,
we represented FB as both an informed search problem and a Q-Learning process.
Adopting the obvious end of maximizing the game's score as our agent's learning goal,
we approached this problem in three phases. First, we adapted an open source version (XXX LINK TO GITHUB PAGE)
of this project into an interface conducive to AI work, redefining the game to be
comprised of familiar objects like states, goals, rewards, successors, and the other
necessary underlying factors. Second, we devised an informed search solution to the problem.
This involved requring the agent to solve a game with deterministic pipes, i.e.,
to figure out an action set that guides the agent through each pipe successfully.
We used the A* algorithm in this phase, with carefully designed heuristic that
served to break the larger problem into subproblems, with the agent only needing
to find a path between one pair of pipes at a time. The final phase of the project
was to relax the constraint of determinism in the pipes and to require the agent
to solve a game ``as it comes''. This involved a shift in paradigm, reforming
the game as an offline policy learning problem rather than as a search problem.
Our results were uniformly positive, with the AI-representation of the game serving
us well, the informed search algorithms able to solve large generated games in
quasilinear time, and the policy derived from Q-learning successful outperforming
any reasonable human.\\\\
This project was a true capstone to our study in this course. From proofs of
heuristic admissibility in informed search to extensive optimization of the policy
derivation algorithm in the Q-learning section, we engaged deeply with the core
tenets of the course throughout our work.


\section{Background and Related Work}



\section{Problem Specification}

% A clear description of the problem you are solving in both general terms
% and how you've mapped it to a formal problem specification.
We will first present an overview of our system's representation of the FB
environment, before specifying the problem formally for both informed search
and Q-Learning.\\\\
\subsection{Parameters of the Flappy Bird World}
The FB world is fully specified by the following:
\begin{enumerate}
    \item The location (in pixel coordinates) of the bird
    \item The locations of the pipes that the bird (or human player) can see
    \item The velocity and acceleration vectors of the bird
    \item The current score of the bird
    \item \emph{(If deterministic)} The offset in the pipe list that is next going to be generated
\end{enumerate}
\subsection{Informed Search}
In general terms, informed search is applied to FB by representing the FB
world as a sequence of nodes, each with a full specification of the world's state,
as outlined above. We define a goal test to ask whether a node's current score
is equal to the score desired (i.e., the size of the game currently being solved).
We also define a function to return the successors of a given node. Combined with
functionality to simulate the game's progress through time given the physical
attributes of each state, the FB world is represented well as an informed
search problem. Formally, the game is comprised of:
\\\\State nodes, each with:
\begin{enumerate}
    \item A location $x \in \reals^2$
    \item A velocity $v \in \reals^2$
    \item An acceleration $a \in \reals^2$
    \item A score $c \in \integers$
    \item A set of pipe locations $P \in (\reals^2)^n$, where $n$ is the
    number of pipes the bird can see
\end{enumerate}
The functions, each operating on a node $s$, with attributes subscripted as above
\begin{enumerate}
    \item $G(s) = \mathbb{1}(s_c = C)$, a goal test that returns true if and only if
    a node's score is equal to the goal score $C$.
    \item $A(s)$, a function returning the actions available to the agent at
        a state. These    will always be a member of $\emph{P} (\mathbb{A})$,
        where $\mathbb{A} = \{\text{FLAP}, \text{FALL}\}$, and $\emph{P}$ denotes
        the power set operator.
    \item $N(s)$, a function that returns a set of neighbors of the node, along
    with the action required to get to each.
    \item $D(s)$, a function returning true if and only if the state represents the
    ``game over'' state (after a crash).
\end{enumerate}
And the constants $C$, the ``goal number'' of nodes that codes the complexity of the
game, and various more mundane constants that denote the number of pixels on the screen
and such. It should be noted that for cleanliness, we abstracted these away into the
functions $G, D, A, N$ so that the agent doesn't ever need to interact with them.

\section{Approach}

% A clear specification of the algorithm(s) you used and a description
% of the main data structures in the implementation. Include a
% discussion of any details of the algorithm that were not in the
% published paper(s) that formed the basis of your implementation. A
% reader should be able to reconstruct and verify your work from reading
% your paper.

\subsection{Informed Search}
We used the well-known A* algorithm for the informed search part of our problem.
Generating a large number of deterministic pipes for the agent to test on, we
encoded world information into state-nodes, as specified above. We then defined
the functions as specified above to allow to agent to ``move'' between states
through time. This was an interesting procedure, as the agent does not directly
control its fate, as Pac-Man did. Rather, it has control over the second- and
third- order physical attributes of its state: its velocity and acceleration.
In this way, the agent's path to the goal is not so much a sequence of choices of
of physical locations, but rather a sequence of mechanical vectors through space
and time. Rather than rewriting the A* algorithm here, the foregoing formal
specification defined the key parts of the algorithm in terms of the FB game.
For example, the canonical Get-Neighbors function is implemented as $N$, which
returns the next state nodes, operating on the current one using the physical
transitions that we encoded in addition to taking into account the range of actions
that the agent has available. It is here that we were able to merge the algorithm
with the physicality of the game to find an efficient solution.
\subsubsection{Heuristic}
The crux of the informed search section of this problem was in the choice of heuristic.
As specified by the A* algorithm, the priority of a node in the frontier is given
by the function $f = g + h$, where $g$ is the foregoing cost to get to the node and
$h$ is the value of the heuristic function that estimates the future cost to the goal.
We experimented extensively with heuristic functions, finding that there is high variance
in the results as a function of the quality of the heuristic. We will present our
final heuristic function, then prove its admissibility and consistency. Define $m(s)$
to be the Manhattan distance between the current state's location and the \emph{midpoint}
of the next pipe that the agent will encounter. This is the coordinate at the x-
and y-averages of the pipe's left and right corners, and upper and lower pipes,
respectively. Then let our heuristic $h(s) = m(s)$. We will now show certain
properties of this heuristic. But first, note an important property of this game's
system: the x-velocity of the agent is constant, which means that every successive
node represents a movement of constant distance in the direction of the goal.
The implication is that every correct solution to a problem will have exactly
the same cost (specifically, the number of pixels traveled to the goal in the x
direction, divided by the velocity in the x direction). \\\\
Admissibility is trivial to show, since the heuristic gives the Manhattan distance to
the next pipe. Because the distance between pipes is greater than the vertical range
of pipe midpoints, $m(s)$ is necessarily less than or equal to the true distance to the
goal. Showing consistency is a simple matter of noting that every successor to a node
will be exactly the same amount closer to the next pipe midpoint as it is closer
to the goal. This is again because of constant x velocity. Since the heuristic
is admissible and consistent, we showed that A* is complete and optimal in this case.
The empirical results for the heuristic will be discussed in the results section.

\subsection{Implementation Notes}
The encoding of states into graph search nodes can be found in \texttt{node_util.py},
while the A* implementation can be found in \texttt{algs.py}. Both implementations
were written independently and internally, without consultation of outside sources
for either. We recommend that the reader open these files, as they are commented
extensively and should be considered part of our explanation.



\section{Experiments}
Analysis, evaluation, and critique of the algorithm and your
implementation. Include a description of the testing data you used and
a discussion of examples that illustrate major features of your
system. Testing is a critical part of system construction, and the
scope of your testing will be an important component in our
evaluation. Discuss what you learned from the implementation.

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    & Score \\
    \midrule
    Approach 1 & \\
    Approach 2 & \\
    \bottomrule
  \end{tabular}
  \caption{Description of the results.}
\end{table}


\subsection{Results}

 For algorithm-comparison projects: a section reporting empirical comparison results preferably presented graphically.


\section{Discussion}

Summary of approach and results. Major takeaways? Things you could improve in future work?

\appendix

\section{System Description}

 Appendix 1 – A clear description of how to use your system and how to generate the output you discussed in the write-up. \emph{The teaching staff must be able to run your system.}

\section{Group Makeup}

 Appendix 2 – A list of each project participant and that
participant’s contributions to the project. If the division of work
varies significantly from the project proposal, provide a brief
explanation.  Your code should be clearly documented.



\bibliographystyle{plain}
\bibliography{project-template}

\end{document}
